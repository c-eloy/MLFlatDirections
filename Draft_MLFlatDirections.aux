\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{Maldacena:1997re}
\citation{Ooguri:2016pdq,Palti:2019pca}
\citation{Giambrone:2021wsm}
\citation{Aharony:2001dp,Dong:2014tsa}
\citation{Eloy:2024lwn}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{Comsa:2019rcz,Berman:2022jqn}
\citation{Ashmore:2019wzb,Berman:2021mcw,Larfors:2022nep,Berglund:2022gvm,Jejjala:2020wcc,Douglas:2006rr,Larfors:2021pbb,He:2018jtw}
\citation{He:2017aed,Carifio:2017bov,Ruehle:2017mzq}
\citation{Chen:2020dxg}
\citation{Brady:2025zzi,Krishnan:2020sfg}
\citation{Hashimoto:2018ftp}
\citation{Ruehle:2020jrk,He:2023csq,Bao:2021auj}
\citation{koza1994genetic}
\citation{virgolin2021improving,randall2022bingo,burlacu2019parsimony}
\citation{petersen2019deep,kamienny2022end}
\citation{valipour2021symbolicgpt}
\citation{bastiani2025diffusion}
\citation{2018arXiv180607259S}
\citation{Udrescu:2019mnk}
\citation{neal1998annealedimportancesampling}
\citation{del2006sequential}
\@writefile{toc}{\contentsline {section}{\numberline {2}Supergravity setup}{3}{section.2}\protected@file@percent }
\newlabel{eq: lagrangian_rephrased}{{2.1}{3}{Supergravity setup}{equation.3}{}}
\newlabel{eq: embtensor_rephrased}{{2.2}{3}{Supergravity setup}{equation.4}{}}
\newlabel{eq: scalarcoset_rephrased}{{2.3}{3}{Supergravity setup}{equation.5}{}}
\newlabel{eq:so84gen_rephrased}{{2.5}{3}{Supergravity setup}{equation.7}{}}
\citation{Eloy:2021fhc}
\citation{Eloy:2021fhc}
\newlabel{eq: scalarpot_rephrased}{{2.8}{4}{Supergravity setup}{equation.10}{}}
\newlabel{eq: gl3gradingbar}{{2.9}{4}{Supergravity setup}{equation.11}{}}
\newlabel{eq: Paulieta}{{2.10}{4}{Supergravity setup}{equation.12}{}}
\newlabel{eq: scalarmatrix}{{2.11}{4}{Supergravity setup}{equation.13}{}}
\newlabel{eq:scalarpotential}{{2.12}{5}{Supergravity setup}{equation.14}{}}
\citation{tensorflow2015-whitepaper}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical analysis}{6}{section.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient descent and local analysis}{6}{subsection.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Sampling the manifold}{6}{subsubsection.18}\protected@file@percent }
\citation{loshchilov2017sgdrstochasticgradientdescent}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Left panel: Histogram of the logarithm of the norm of the gradient for each point. Right panel: Histogram of the distance of the value of the potential for each point.\relax }}{7}{figure.caption.21}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Analyze_V_points}{{1}{7}{Left panel: Histogram of the logarithm of the norm of the gradient for each point. Right panel: Histogram of the distance of the value of the potential for each point.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Evolution of the loss function during gradient descent and corresponding learning rate schedule, noted $\alpha $ on the left y-axis, both plotted on a logarithmic scale. Dashed lines indicate epochs at which the optimizer was reinitialized.\relax }}{8}{figure.caption.22}\protected@file@percent }
\newlabel{fig:loss_grad_des}{{2}{8}{Evolution of the loss function during gradient descent and corresponding learning rate schedule, noted $\alpha $ on the left y-axis, both plotted on a logarithmic scale. Dashed lines indicate epochs at which the optimizer was reinitialized.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Determining the dimension}{8}{subsubsection.24}\protected@file@percent }
\citation{10.1007/978-3-642-37456-2_14}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Triangular plot showing $2d$ projections and $1d$ histograms of the data after the gradient descent\relax }}{9}{figure.caption.23}\protected@file@percent }
\newlabel{triangular_plot_1_2_4_8_10}{{3}{9}{Triangular plot showing $2d$ projections and $1d$ histograms of the data after the gradient descent\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Results of the local PCA analysis. The $x$-axis shows the dimension inferred by the algorithm, and the $y$-axis indicates the proportion of points for which that dimension was found. Each curve corresponds to a different value of $k$ in the $k$-nearest neighbours.\relax }}{10}{figure.caption.25}\protected@file@percent }
\newlabel{local_pca}{{4}{10}{Results of the local PCA analysis. The $x$-axis shows the dimension inferred by the algorithm, and the $y$-axis indicates the proportion of points for which that dimension was found. Each curve corresponds to a different value of $k$ in the $k$-nearest neighbours.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 3D plots of the data in selected coordinates. Left: $(x_4, x_8, x_{10})$. Right: $(x_2, x_4, x_8)$.\relax }}{11}{figure.caption.27}\protected@file@percent }
\newlabel{3dplots}{{5}{11}{3D plots of the data in selected coordinates. Left: $(x_4, x_8, x_{10})$. Right: $(x_2, x_4, x_8)$.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Annealed Importance Sampling for polynomial symbolic regression}{11}{subsection.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Annealed Sequential Monte Carlo principle}{12}{subsubsection.29}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The flow chart of the Annealed Importance Sampling - Sequential Monte Carlo algorithm\relax }}{12}{figure.caption.30}\protected@file@percent }
\newlabel{ASMCflowchart}{{6}{12}{The flow chart of the Annealed Importance Sampling - Sequential Monte Carlo algorithm\relax }{figure.caption.30}{}}
\newlabel{eq:gamman}{{3.2}{13}{Annealed Sequential Monte Carlo principle}{equation.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Results}{14}{subsubsection.37}\protected@file@percent }
\newlabel{eq:pols}{{3.8}{14}{Results}{equation.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Setup of ASMC}{15}{subsubsection.47}\protected@file@percent }
\newlabel{eq:sample10000adaptativetemp}{{3.10}{16}{Setup of ASMC}{equation.49}{}}
\newlabel{table:results}{{3.12}{16}{Setup of ASMC}{equation.51}{}}
\citation{Udrescu:2019mnk}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {\bfseries  \boldmath  \texttt  {acceptance\_ratio} = 0.8 - 0.5 $\times $ (\texttt  {epoch}/\texttt  {n\_epochs})} (a) Evolution of the mean number of particles reproducing each target polynomial (log scale). (b) Evolution of the mean number of particles reproducing a target polynomial, along with its $1\sigma $ deviation, for the parameters described in~\eqref  {eq:sample10000adaptativetemp}.\relax }}{17}{figure.caption.52}\protected@file@percent }
\newlabel{fig:evolutionoftargetreproduction}{{7}{17}{{\bfseries \boldmath \texttt {acceptance\_ratio} = 0.8 - 0.5 $\times $ (\texttt {epoch}/\texttt {n\_epochs})} (a) Evolution of the mean number of particles reproducing each target polynomial (log scale). (b) Evolution of the mean number of particles reproducing a target polynomial, along with its $1\sigma $ deviation, for the parameters described in~\eqref {eq:sample10000adaptativetemp}.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Pie chart showing the repartition of the number of distinct polynomials found in the runs. Note that there is also the failure case, which corresponds 3 different runs.\relax }}{18}{figure.caption.58}\protected@file@percent }
\newlabel{fig:piechart}{{8}{18}{Pie chart showing the repartition of the number of distinct polynomials found in the runs. Note that there is also the failure case, which corresponds 3 different runs.\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{18}{section.59}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Supergravity solutions}{19}{appendix.60}\protected@file@percent }
\newlabel{eq:solvepol}{{A.1}{19}{Supergravity solutions}{equation.61}{}}
\newlabel{eq:rulex8x10}{{A.3}{19}{Supergravity solutions}{equation.63}{}}
\@writefile{toc}{\contentsline {paragraph}{Bosonic spectrum}{20}{paragraph*.66}\protected@file@percent }
\bibdata{references}
\bibcite{Maldacena:1997re}{1}
\bibcite{Ooguri:2016pdq}{2}
\bibcite{Palti:2019pca}{3}
\bibcite{Giambrone:2021wsm}{4}
\bibcite{Aharony:2001dp}{5}
\@writefile{toc}{\contentsline {section}{\numberline {B}Details on the algorithm}{21}{appendix.70}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Local Dimension Estimation using PCA\relax }}{21}{algorithm.71}\protected@file@percent }
\newlabel{alg:local-dim}{{1}{21}{Local Dimension Estimation using PCA\relax }{algorithm.71}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Annealing Importance Sampling for Polynomial Discovery\relax }}{22}{algorithm.72}\protected@file@percent }
\newlabel{alg:annealing-is}{{2}{22}{Annealing Importance Sampling for Polynomial Discovery\relax }{algorithm.72}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Initialize Sparse Polynomials\relax }}{23}{algorithm.73}\protected@file@percent }
\newlabel{alg:initialize-sparse}{{3}{23}{Initialize Sparse Polynomials\relax }{algorithm.73}{}}
\bibcite{Dong:2014tsa}{6}
\bibcite{Eloy:2024lwn}{7}
\bibcite{Comsa:2019rcz}{8}
\bibcite{Berman:2022jqn}{9}
\bibcite{Ashmore:2019wzb}{10}
\bibcite{Berman:2021mcw}{11}
\bibcite{Larfors:2022nep}{12}
\bibcite{Berglund:2022gvm}{13}
\bibcite{Jejjala:2020wcc}{14}
\bibcite{Douglas:2006rr}{15}
\bibcite{Larfors:2021pbb}{16}
\bibcite{He:2018jtw}{17}
\bibcite{He:2017aed}{18}
\bibcite{Carifio:2017bov}{19}
\bibcite{Ruehle:2017mzq}{20}
\bibcite{Chen:2020dxg}{21}
\bibcite{Brady:2025zzi}{22}
\bibcite{Krishnan:2020sfg}{23}
\bibcite{Hashimoto:2018ftp}{24}
\bibcite{Ruehle:2020jrk}{25}
\bibcite{He:2023csq}{26}
\bibcite{Bao:2021auj}{27}
\bibcite{koza1994genetic}{28}
\bibcite{virgolin2021improving}{29}
\bibcite{randall2022bingo}{30}
\bibcite{burlacu2019parsimony}{31}
\bibcite{petersen2019deep}{32}
\bibcite{kamienny2022end}{33}
\bibcite{valipour2021symbolicgpt}{34}
\bibcite{bastiani2025diffusion}{35}
\bibcite{2018arXiv180607259S}{36}
\bibcite{Udrescu:2019mnk}{37}
\bibcite{neal1998annealedimportancesampling}{38}
\bibcite{del2006sequential}{39}
\bibcite{Eloy:2021fhc}{40}
\bibcite{tensorflow2015-whitepaper}{41}
\bibcite{loshchilov2017sgdrstochasticgradientdescent}{42}
\bibcite{10.1007/978-3-642-37456-2_14}{43}
\ttl@finishall
\gdef \@abspage@last{27}
