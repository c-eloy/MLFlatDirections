\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{JHEP}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{Maldacena:1997re}
\citation{Ooguri:2016pdq,Palti:2019pca}
\citation{Giambrone:2021wsm}
\citation{Aharony:2001dp,Dong:2014tsa}
\citation{Eloy:2024lwn}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{Comsa:2019rcz,Berman:2022jqn}
\citation{Ashmore:2019wzb,Berman:2021mcw,Larfors:2022nep,Berglund:2022gvm,Jejjala:2020wcc,Douglas:2006rr,Larfors:2021pbb,He:2018jtw}
\citation{He:2017aed,Carifio:2017bov,Ruehle:2017mzq}
\citation{Chen:2020dxg}
\citation{Brady:2025zzi,Krishnan:2020sfg}
\citation{Hashimoto:2018ftp}
\citation{Ruehle:2020jrk,He:2023csq,Bao:2021auj}
\citation{koza1994genetic,randall2022bingo,burlacu2019parsimony}
\citation{petersen2019deep,kamienny2022end}
\citation{valipour2021symbolicgpt}
\citation{bastiani2025diffusion}
\citation{neal1998annealedimportancesampling}
\citation{del2006sequential}
\@writefile{toc}{\contentsline {section}{\numberline {2}Supergravity setup}{3}{section.2}\protected@file@percent }
\newlabel{eq: lagrangian_rephrased}{{2.1}{3}{Supergravity setup}{equation.3}{}}
\newlabel{eq: embtensor_rephrased}{{2.2}{3}{Supergravity setup}{equation.4}{}}
\newlabel{eq: scalarcoset_rephrased}{{2.3}{3}{Supergravity setup}{equation.5}{}}
\newlabel{eq:so84gen_rephrased}{{2.5}{3}{Supergravity setup}{equation.7}{}}
\citation{Eloy:2021fhc}
\citation{Eloy:2021fhc}
\newlabel{eq: scalarpot_rephrased}{{2.8}{4}{Supergravity setup}{equation.10}{}}
\newlabel{eq: gl3gradingbar}{{2.9}{4}{Supergravity setup}{equation.11}{}}
\newlabel{eq: Paulieta}{{2.10}{4}{Supergravity setup}{equation.12}{}}
\newlabel{eq: scalarmatrix}{{2.11}{4}{Supergravity setup}{equation.13}{}}
\newlabel{eq:scalarpotential}{{2.12}{4}{Supergravity setup}{equation.14}{}}
\citation{tensorflow2015-whitepaper}
\citation{loshchilov2017sgdrstochasticgradientdescent}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical analysis}{6}{section.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient descent and local analysis}{6}{subsection.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Sampling the manifold}{6}{subsubsection.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Left panel: Histogram of the logarithm of the norm of the gradient for each point. Right panel: Histogram of the distance of the value of the potential for each point.\relax }}{7}{figure.caption.20}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Analyze_V_points}{{1}{7}{Left panel: Histogram of the logarithm of the norm of the gradient for each point. Right panel: Histogram of the distance of the value of the potential for each point.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Determining the dimension}{7}{subsubsection.23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Evolution of the loss function during gradient descent and corresponding learning rate schedule, noted $\alpha $ on the left y-axis, both plotted on a logarithmic scale. Dashed lines indicate epochs at which the optimizer was reinitialized.\relax }}{8}{figure.caption.21}\protected@file@percent }
\newlabel{fig:loss_grad_des}{{2}{8}{Evolution of the loss function during gradient descent and corresponding learning rate schedule, noted $\alpha $ on the left y-axis, both plotted on a logarithmic scale. Dashed lines indicate epochs at which the optimizer was reinitialized.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Triangular plot showing $2d$ projections and $1d$ histograms of the data after the gradient descent\relax }}{9}{figure.caption.22}\protected@file@percent }
\newlabel{triangular_plot_1_2_4_8_10}{{3}{9}{Triangular plot showing $2d$ projections and $1d$ histograms of the data after the gradient descent\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Results of the local PCA analysis. The $x$-axis shows the dimension inferred by the algorithm, and the $y$-axis indicates the proportion of points for which that dimension was found. Each curve corresponds to a different value of $k$ in the $k$-nearest neighbours.\relax }}{10}{figure.caption.24}\protected@file@percent }
\newlabel{local_pca}{{4}{10}{Results of the local PCA analysis. The $x$-axis shows the dimension inferred by the algorithm, and the $y$-axis indicates the proportion of points for which that dimension was found. Each curve corresponds to a different value of $k$ in the $k$-nearest neighbours.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 3D plots of the data in selected coordinates. Left: $(x_4, x_8, x_{10})$. Right: $(x_2, x_4, x_8)$.\relax }}{10}{figure.caption.26}\protected@file@percent }
\newlabel{3dplots}{{5}{10}{3D plots of the data in selected coordinates. Left: $(x_4, x_8, x_{10})$. Right: $(x_2, x_4, x_8)$.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Annealed Importance Sampling for polynomial symbolic regression}{11}{subsection.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Annealed Sequential Monte Carlo principle}{11}{subsubsection.28}\protected@file@percent }
\newlabel{eq:gamman}{{3.2}{12}{Annealed Sequential Monte Carlo principle}{equation.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Results}{15}{subsubsection.35}\protected@file@percent }
\newlabel{eq:pols}{{3.8}{15}{Results}{equation.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Setup of ASMC}{16}{subsubsection.45}\protected@file@percent }
\newlabel{eq:sample10000adaptativetemp}{{3.10}{16}{Setup of ASMC}{equation.47}{}}
\newlabel{table:results}{{3.12}{16}{Setup of ASMC}{equation.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{16}{section.51}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {\bfseries  \boldmath  \texttt  {acceptance\_ratio} = 0.8 - 0.5 $\times $ (\texttt  {epoch}/\texttt  {n\_epochs})} (a) Evolution of the mean number of particles reproducing each target polynomial (log scale). (b) Evolution of the mean number of particles reproducing a target polynomial, along with its $1\sigma $ deviation, for the parameters described in~\eqref  {eq:sample10000adaptativetemp}.\relax }}{17}{figure.caption.50}\protected@file@percent }
\newlabel{fig:evolutionoftargetreproduction}{{6}{17}{{\bfseries \boldmath \texttt {acceptance\_ratio} = 0.8 - 0.5 $\times $ (\texttt {epoch}/\texttt {n\_epochs})} (a) Evolution of the mean number of particles reproducing each target polynomial (log scale). (b) Evolution of the mean number of particles reproducing a target polynomial, along with its $1\sigma $ deviation, for the parameters described in~\eqref {eq:sample10000adaptativetemp}.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Supergravity solutions}{18}{appendix.52}\protected@file@percent }
\newlabel{eq:solvepol}{{A.1}{18}{Supergravity solutions}{equation.53}{}}
\newlabel{eq:rulex8x10}{{A.3}{18}{Supergravity solutions}{equation.55}{}}
\bibdata{references}
\bibcite{Maldacena:1997re}{1}
\bibcite{Ooguri:2016pdq}{2}
\bibcite{Palti:2019pca}{3}
\bibcite{Giambrone:2021wsm}{4}
\bibcite{Aharony:2001dp}{5}
\bibcite{Dong:2014tsa}{6}
\bibcite{Eloy:2024lwn}{7}
\@writefile{toc}{\contentsline {paragraph}{Bosonic spectrum}{19}{paragraph*.58}\protected@file@percent }
\bibcite{Comsa:2019rcz}{8}
\bibcite{Berman:2022jqn}{9}
\bibcite{Ashmore:2019wzb}{10}
\bibcite{Berman:2021mcw}{11}
\bibcite{Larfors:2022nep}{12}
\bibcite{Berglund:2022gvm}{13}
\bibcite{Jejjala:2020wcc}{14}
\bibcite{Douglas:2006rr}{15}
\bibcite{Larfors:2021pbb}{16}
\bibcite{He:2018jtw}{17}
\bibcite{He:2017aed}{18}
\bibcite{Carifio:2017bov}{19}
\bibcite{Ruehle:2017mzq}{20}
\bibcite{Chen:2020dxg}{21}
\bibcite{Brady:2025zzi}{22}
\bibcite{Krishnan:2020sfg}{23}
\bibcite{Hashimoto:2018ftp}{24}
\bibcite{Ruehle:2020jrk}{25}
\bibcite{He:2023csq}{26}
\bibcite{Bao:2021auj}{27}
\bibcite{koza1994genetic}{28}
\bibcite{randall2022bingo}{29}
\bibcite{burlacu2019parsimony}{30}
\bibcite{petersen2019deep}{31}
\bibcite{kamienny2022end}{32}
\bibcite{valipour2021symbolicgpt}{33}
\bibcite{bastiani2025diffusion}{34}
\bibcite{neal1998annealedimportancesampling}{35}
\bibcite{del2006sequential}{36}
\bibcite{Eloy:2021fhc}{37}
\bibcite{tensorflow2015-whitepaper}{38}
\bibcite{loshchilov2017sgdrstochasticgradientdescent}{39}
\ttl@finishall
\gdef \@abspage@last{22}
